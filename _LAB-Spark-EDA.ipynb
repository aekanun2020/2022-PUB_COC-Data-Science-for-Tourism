{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "2.codeTemplate-PySpark-onColab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aekanun2020/2022-PUB_COC-Data-Science-for-Tourism/blob/main/1_codeTemplate_PySpark_onColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dTkA-bDa5bm"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy4_WOqEa5GJ"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.1-bin-hadoop2.7\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI7zVpqja4du"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gLOiqgf1a2Ku"
      },
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GBqMbCCxa2Kv"
      },
      "source": [
        "spark = SparkSession.builder \\\n",
        "   .appName(\"Neural Network Model\") \\\n",
        "   .config(\"spark.executor.memory\", \"3gb\") \\\n",
        "   .getOrCreate()\n",
        "   \n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjoDgjaja2Kv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "3a7f1f1d-7db8-4bfd-e342-8cdef1327661"
      },
      "source": [
        "sc"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://782b88520b27:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7c5vxPNAPVi"
      },
      "source": [],
      "execution_count": 6,
      "outputs": []
    }
  ]
"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from pyspark.sql import functions as sparkf\n","\n","raw_df = spark.createDataFrame(\n"," [\n","  ('Store 1',1,448),\n","  ('Store 1',2,None),\n","  ('Store 1',3,499),\n","  ('Store 1',44,432),\n","  (None,None,None),\n","  ('Store 2',1,355),\n","  ('Store 2',1,355),\n","  ('Store 2',None,345),\n","  ('Store 2',3,387),\n","  ('Store 2',4,312)\n","],\n"," ['Store','WeekInMonth','Revenue']\n",")\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Store: string (nullable = true)\n"," |-- WeekInMonth: long (nullable = true)\n"," |-- Revenue: long (nullable = true)\n","\n"]}],"source":["raw_df.printSchema()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["10"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["raw_df.count()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-----------+-------+\n","|  Store|WeekInMonth|Revenue|\n","+-------+-----------+-------+\n","|Store 1|          1|    448|\n","|Store 1|          2|   null|\n","|Store 1|          3|    499|\n","|Store 1|         44|    432|\n","|   null|       null|   null|\n","|Store 2|          1|    355|\n","|Store 2|          1|    355|\n","|Store 2|       null|    345|\n","|Store 2|          3|    387|\n","|Store 2|          4|    312|\n","+-------+-----------+-------+\n","\n"]}],"source":["raw_df.show()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------+------------------+------------------+\n","|summary|  Store|       WeekInMonth|           Revenue|\n","+-------+-------+------------------+------------------+\n","|  count|      9|                 8|                 8|\n","|   mean|   null|             7.375|           391.625|\n","| stddev|   null|14.841423689890979|62.741960213469355|\n","|    min|Store 1|                 1|               312|\n","|    max|Store 2|                44|               499|\n","+-------+-------+------------------+------------------+\n","\n"]}],"source":["raw_df.describe().show()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+-----------+-------+\n","|Store|WeekInMonth|Revenue|\n","+-----+-----------+-------+\n","|    1|          2|      2|\n","+-----+-----------+-------+\n","\n"]}],"source":["raw_df.select(\n","  [sparkf.count(sparkf.when(sparkf.isnull(c), c)).alias(c) for c in raw_df.columns]\n",").show()\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-----------+-------+\n","|  Store|WeekInMonth|Revenue|\n","+-------+-----------+-------+\n","|Store 1|          2|   null|\n","|   null|       null|   null|\n","|Store 2|       null|    345|\n","+-------+-----------+-------+\n","\n"]}],"source":["#ระบุว่า Nulls อยู่ที่ row ไหน - เพื่อดูว่าเป็น MCAR / MAR / MNAR / Structural Missing\n","\n","from functools import reduce\n","\n","raw_df.filter(reduce(lambda a1, a2: a1 | a2,\\\n","                     (sparkf.col(c).isNull() \\\n","                      for c in raw_df.columns))).show()\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-----------+-------+\n","|  Store|WeekInMonth|Revenue|\n","+-------+-----------+-------+\n","|Store 1|          1|    448|\n","|Store 1|          2|   null|\n","|Store 1|          3|    499|\n","|Store 1|         44|    432|\n","|Store 2|          1|    355|\n","|Store 2|          1|    355|\n","|Store 2|       null|    345|\n","|Store 2|          3|    387|\n","|Store 2|          4|    312|\n","+-------+-----------+-------+\n","\n"]}],"source":["#ซ่อน/ลบ เฉพาะ row ที่มีค่า nulls ในทุก attributes แล้วซ่อนค่า nulls จากนั้นส่งให้กับตัวแปรใหม่ [III]\n","\n","noNullRow_Df = raw_df.dropna('all')\n","\n","noNullRow_Df.show()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-----------+-------+\n","|  Store|WeekInMonth|Revenue|\n","+-------+-----------+-------+\n","|Store 1|          1|    448|\n","|Store 1|          2|   null|\n","|Store 1|          3|    499|\n","|Store 1|         44|    432|\n","|Store 2|          1|    355|\n","|Store 2|          1|    355|\n","|Store 2|          3|    387|\n","|Store 2|          4|    312|\n","+-------+-----------+-------+\n","\n"]}],"source":["#ซ่อน/ลบ row ที่มีค่า nulls ใน attributes ใด attribute หนึ่ง (หรือทุก attributes) จาก subset [IV]\n","\n","\n","raw_df.dropna(how='any', \\\n","              subset=['Store','WeekInMonth']).show()\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-----------+-------+\n","|  Store|WeekInMonth|Revenue|\n","+-------+-----------+-------+\n","|Store 1|          1|    448|\n","|Store 1|          3|    499|\n","|Store 1|         44|    432|\n","|Store 2|          1|    355|\n","|Store 2|          1|    355|\n","|Store 2|          3|    387|\n","|Store 2|          4|    312|\n","+-------+-----------+-------+\n","\n"]}],"source":["#ซ่อน/ลบ row ที่มีค่า nulls ใน attributes ใด attribute หนึ่ง (หรือทุก attributes) แล้วส่งให้กับตัวแปรใหม่ [V]\n","\n","raw_df.dropna(how='any').show()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-----------+-------+\n","|  Store|WeekInMonth|Revenue|\n","+-------+-----------+-------+\n","|Store 1|          1|    448|\n","|Store 1|          2|   9999|\n","|Store 1|          3|    499|\n","|Store 1|         44|    432|\n","|   null|       null|   9999|\n","|Store 2|          1|    355|\n","|Store 2|          1|    355|\n","|Store 2|       null|    345|\n","|Store 2|          3|    387|\n","|Store 2|          4|    312|\n","+-------+-----------+-------+\n","\n"]}],"source":["#แทนที่ค่า nulls ด้วยเลข 9999 เฉพาะใน “Revenue” [VI]\n","\n","raw_df.fillna(9999,['Revenue']).show()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------+-----------+-------+\n","|         Store|WeekInMonth|Revenue|\n","+--------------+-----------+-------+\n","|       Store 1|          1|    448|\n","|       Store 1|          2|      3|\n","|       Store 1|          3|    499|\n","|       Store 1|         44|    432|\n","|Assume_Store 1|          2|      3|\n","|       Store 2|          1|    355|\n","|       Store 2|          1|    355|\n","|       Store 2|          2|    345|\n","|       Store 2|          3|    387|\n","|       Store 2|          4|    312|\n","+--------------+-----------+-------+\n","\n"]}],"source":["#แทนที่ค่า nulls ด้วยเลข 0 ในทุก Attributes ซึ่งเป็น numeric และด้วยค่าอื่นๆ สำหรับ string [VII]\n","raw_df.fillna({'Store':'Assume_Store 1',\\\n","              'WeekInMonth':'2','Revenue':3}).show()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# [VIII]\n","from pyspark.sql import functions as sparkf"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-----------+-------+\n","|  Store|WeekInMonth|Revenue|\n","+-------+-----------+-------+\n","|Store 1|          1|    448|\n","|Store 1|          2|   null|\n","|Store 1|          3|    499|\n","|Store 1|         44|    432|\n","|   null|       null|   null|\n","|Store 2|          1|    355|\n","|Store 2|          1|    355|\n","|Store 2|       null|    345|\n","|Store 2|          3|    387|\n","|Store 2|          4|    312|\n","+-------+-----------+-------+\n","\n"]}],"source":["raw_df.show()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Store: string (nullable = true)\n"," |-- WeekInMonth: long (nullable = true)\n"," |-- Revenue: long (nullable = true)\n","\n"]}],"source":["raw_df.printSchema()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["magic_percentile = sparkf.expr('percentile_approx(Revenue, 0.5)')"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-----------+-------+\n","|  Store|WeekInMonth|Revenue|\n","+-------+-----------+-------+\n","|Store 2|          4|    312|\n","|Store 2|          1|    355|\n","|Store 2|          1|    355|\n","|Store 2|          3|    387|\n","|Store 1|         44|    432|\n","|Store 1|          1|    448|\n","|Store 1|          3|    499|\n","+-------+-----------+-------+\n","\n"]}],"source":["raw_df.na.drop().orderBy('Revenue').show()"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-------+\n","|  Store|med_val|\n","+-------+-------+\n","|Store 1|    448|\n","+-------+-------+\n","\n"]}],"source":["raw_df.na.drop().groupBy('Store').agg(magic_percentile.alias('med_val'))\\\n",".filter(sparkf.col('Store') == 'Store 1').show()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["imputed_value = raw_df.na.drop().groupBy('Store').agg(magic_percentile.alias('med_val'))\\\n",".filter(sparkf.col('Store') == 'Store 1').collect()[0][1]"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["448"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["imputed_value"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-----------+-------+--------------+\n","|  Store|WeekInMonth|Revenue|noNull_Revenue|\n","+-------+-----------+-------+--------------+\n","|Store 1|          1|    448|           448|\n","|Store 1|          2|   null|           448|\n","|Store 1|          3|    499|           499|\n","|Store 1|         44|    432|           432|\n","|   null|       null|   null|          null|\n","|Store 2|          1|    355|           355|\n","|Store 2|          1|    355|           355|\n","|Store 2|       null|    345|           345|\n","|Store 2|          3|    387|           387|\n","|Store 2|          4|    312|           312|\n","+-------+-----------+-------+--------------+\n","\n"]}],"source":["raw_df.withColumn('noNull_Revenue'\\\n","                  ,sparkf.when((sparkf.col('Store')=='Store 1')\\\n","                &(sparkf.col('Revenue').isNull()),imputed_value).otherwise(sparkf.col('Revenue'))).show()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-----------+-------+\n","|  Store|WeekInMonth|Revenue|\n","+-------+-----------+-------+\n","|Store 1|          1|    448|\n","|Store 1|          2|   null|\n","|Store 1|          3|    499|\n","|Store 1|         44|    432|\n","|   null|       null|   null|\n","|Store 2|          1|    355|\n","|Store 2|          1|    355|\n","|Store 2|       null|    345|\n","|Store 2|          3|    387|\n","|Store 2|          4|    312|\n","+-------+-----------+-------+\n","\n"]}],"source":["raw_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":2}
